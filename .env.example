# NVIDIA NIM Config
NVIDIA_NIM_API_KEY=""


# OpenRouter Config
OPENROUTER_API_KEY=""


# LM Studio Config (local provider, no API key required)
LM_STUDIO_BASE_URL="http://localhost:1234/v1"


# All Claude model requests are mapped to this model
# Format: provider_type/model/name
# Valid providers: "nvidia_nim" | "open_router" | "lmstudio"
MODEL="nvidia_nim/stepfun-ai/step-3.5-flash"


# Provider Config
PROVIDER_RATE_LIMIT=40
PROVIDER_RATE_WINDOW=60
PROVIDER_MAX_CONCURRENCY=5


# HTTP client timeouts (seconds) for provider API requests
HTTP_READ_TIMEOUT=300
HTTP_WRITE_TIMEOUT=10
HTTP_CONNECT_TIMEOUT=2


# Messaging Platform: "telegram" | "discord"
MESSAGING_PLATFORM=discord
MESSAGING_RATE_LIMIT=1
MESSAGING_RATE_WINDOW=1


# Voice Note Transcription
VOICE_NOTE_ENABLED=false
# TRANSCRIPTION_PROVIDER: "whisper" | "nvidia_riva"
# - "whisper": Hugging Face transformers Whisper (offline, free; install with: uv sync --extra voice)
# - "nvidia_riva": NVIDIA RIVA ASR (requires NVIDIA_RIVA_SERVER and riva client)
TRANSCRIPTION_PROVIDER="whisper"
# NVIDIA RIVA server address (for nvidia_riva provider)
# Use "localhost:50051" for local RIVA, or "riva-ngai.example.com:443" for remote
NVIDIA_RIVA_SERVER="localhost:50051"
# WHISPER_MODEL: Hugging Face ID or short name (tiny, base, small, medium, large-v2, large-v3, large-v3-turbo)
WHISPER_MODEL="base"
# WHISPER_DEVICE: "cpu" | "cuda" (only for whisper provider)
WHISPER_DEVICE="cpu"
HF_TOKEN=""


# Telegram Config
TELEGRAM_BOT_TOKEN=""
ALLOWED_TELEGRAM_USER_ID=""


# Discord Config
DISCORD_BOT_TOKEN=""
ALLOWED_DISCORD_CHANNELS=""


# Agent Config
CLAUDE_WORKSPACE="./agent_workspace"
ALLOWED_DIR=""
FAST_PREFIX_DETECTION=true
ENABLE_NETWORK_PROBE_MOCK=true
ENABLE_TITLE_GENERATION_SKIP=true
ENABLE_SUGGESTION_MODE_SKIP=true
ENABLE_FILEPATH_EXTRACTION_MOCK=true